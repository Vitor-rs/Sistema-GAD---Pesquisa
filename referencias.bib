
@misc{noauthor_tutorial_2020,
	title = {Tutorial sistec},
	url = {https://www.gov.br/mec/pt-br/acesso-a-informacao/institucional/estrutura-organizacional/orgaos-especificos-singulares/secretaria-de-educacao-profissional/TutorialrgoValidadorCiclodeMatrcula.pdf},
	language = {en},
	urldate = {2025-07-20},
	author = {, Sistec},
	month = dec,
	year = {2020},
	keywords = {/unread},
}

@article{harraj_ocr_2015,
	title = {{OCR} accuracy improvement on document images through a novel pre-processing approach},
	volume = {6},
	issn = {2229-3922, 0976-710X},
	url = {http://arxiv.org/abs/1509.03456},
	doi = {10.5121/sipij.2015.6401},
	abstract = {Digital camera and mobile document image acquisition are new trends arising in the world of Optical Character Recognition and text detection. In some cases, such process integrates many distortions and produces poorly scanned text or text-photo images and natural images, leading to an unreliable OCR digitization. In this paper, we present a novel nonparametric and unsupervised method to compensate for undesirable document image distortions aiming to optimally improve OCR accuracy. Our approach relies on a very efficient stack of document image enhancing techniques to recover deformation of the entire document image. First, we propose a local brightness and contrast adjustment method to effectively handle lighting variations and the irregular distribution of image illumination. Second, we use an optimized greyscale conversion algorithm to transform our document image to greyscale level. Third, we sharpen the useful information in the resulting greyscale image using Un-sharp Masking method. Finally, an optimal global binarization approach is used to prepare the final document image to OCR recognition. The proposed approach can significantly improve text detection rate and optical character recognition accuracy. To demonstrate the efficiency of our approach, an exhaustive experimentation on a standard dataset is presented.},
	language = {en},
	number = {4},
	urldate = {2025-07-17},
	journal = {Signal \& Image Processing : an International Journal},
	author = {Harraj, Abdeslam El and Raissouni, Naoufal},
	month = aug,
	year = {2015},
	note = {arXiv:1509.03456 [cs]
OpenAlex: W4230800876
TLDR: A novel nonparametric and unsupervised method to compensate for undesirable document image distortions aiming to optimally improve OCR accuracy and text detection rate is presented.},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
	pages = {1--18},
}

@article{kabashi_trustworthy_2024,
	title = {Trustworthy verification of academic credentials through blockchain technology},
	volume = {20},
	copyright = {Copyright (c) 2024 Faton Kabashi, Halil Snopçe, Artan Luma, Vehbi Neziri},
	issn = {2626-8493},
	url = {https://online-journals.org/index.php/i-joe/article/view/48999},
	doi = {10.3991/ijoe.v20i09.48999},
	abstract = {The increasing demand for a secure and transparent mechanism for verifying academic credentials has led to the exploration of blockchain technology (BT) as a viable solution. This paper presents a three-layered application for the reliable verification of academic credentials, leveraging the immutable and decentralized nature of BT. The system includes a data layer (DL), a blockchain layer (BL), and an application layer (AL). The DL records student records, faculty staff records, and credentials. The BL utilizes smart contracts to record and verify academic records, ensuring their authenticity and integrity. The AL provides a userfriendly interface for various stakeholders, such as students, educational institutions, and employers, to communicate with the system. This effort aims to determine how BT can be utilized to enhance the security, transparency, and efficiency of academic credential verification processes. Efficiency in academic credential verification processes ultimately contributes to a more reliable and effective educational ecosystem.},
	language = {en},
	number = {9},
	urldate = {2025-07-17},
	journal = {International Journal of Online and Biomedical Engineering (ijoe)},
	author = {Kabashi, Faton and Snopçe, Halil and Luma, Artan and Neziri, Vehbi},
	month = jun,
	year = {2024},
	note = {Number: 09
TLDR: This effort aims to determine how BT can be utilized to enhance the security, transparency, and efficiency of academic credential verification processes.},
	keywords = {/unread, Blockchain, academic credentials, decentralization, smart contracts, verification},
	pages = {51--64},
}

@article{ehrmann_named_2024,
	title = {Named entity recognition and classification in historical documents: a survey},
	volume = {56},
	copyright = {https://creativecommons.org/licenses/by-sa/4.0/},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Named entity recognition and classification in historical documents},
	url = {https://dl.acm.org/doi/10.1145/3604931},
	doi = {10.1145/3604931},
	abstract = {After decades of massive digitisation, an unprecedented number of historical documents are available in digital format, along with their machine-readable texts. While this represents a major step forward with respect to preservation and accessibility, it also opens up new opportunities in terms of content mining and the next fundamental challenge is to develop appropriate technologies to efficiently search, retrieve, and explore information from this ‘big data of the past’. Among semantic indexing opportunities, the recognition and classification of named entities are in great demand among humanities scholars. Yet, named entity recognition (NER) systems are heavily challenged with diverse, historical, and noisy inputs. In this survey, we present the array of challenges posed by historical documents to NER, inventory existing resources, describe the main approaches deployed so far, and identify key priorities for future developments.},
	language = {en},
	number = {2},
	urldate = {2025-07-17},
	journal = {ACM Computing Surveys},
	author = {Ehrmann, Maud and Hamdi, Ahmed and Pontes, Elvys Linhares and Romanello, Matteo and Doucet, Antoine},
	month = feb,
	year = {2024},
	note = {Publisher: Association for Computing Machinery (ACM)
MAG: 3199662997
OpenAlex: W3199662997
QID: Q130837343
TLDR: The array of challenges posed by historical documents to NER are presented, existing resources are inventoryed, the main approaches deployed so far are described, and key priorities for future developments are identified.},
	pages = {47},
}

@article{hamdi_-depth_2023,
	title = {In-depth analysis of the impact of {OCR} errors on named entity recognition and linking},
	volume = {29},
	copyright = {https://www.cambridge.org/core/terms},
	issn = {1351-3249, 1469-8110},
	url = {https://www.cambridge.org/core/journals/natural-language-engineering/article/abs/indepth-analysis-of-the-impact-of-ocr-errors-on-named-entity-recognition-and-linking/C732399FF72BAFE8FF830BB1F5ED7576#},
	doi = {10.1017/S1351324922000110},
	abstract = {Named entities (NEs) are among the most relevant type of information that can be used to properly index digital documents and thus easily retrieve them. It has long been observed that NEs are key to accessing the contents of digital library portals as they are contained in most user queries. However, most digitized documents are indexed through their optical character recognition (OCRed) version which include numerous errors. Although OCR engines have considerably improved over the last few years, OCR errors still considerably impact document access. Previous works were conducted to evaluate the impact of OCR errors on named entity recognition (NER) and named entity linking (NEL) techniques separately. In this article, we experimented with a variety of OCRed documents with different levels and types of OCR noise to assess in depth the impact of OCR on named entity processing. We provide a deep analysis of OCR errors that impact the performance of NER and NEL. We then present the resulting exhaustive study and subsequent recommendations on the adequate documents, the OCR quality levels, and the post-OCR correction strategies required to perform reliable NER and NEL.},
	language = {en},
	number = {2},
	urldate = {2025-07-17},
	journal = {Natural Language Engineering},
	author = {Hamdi, Ahmed and Pontes, Elvys Linhares and Sidere, Nicolas and Coustaty, Mickaël and Doucet, Antoine},
	month = mar,
	year = {2023},
	note = {OpenAlex: W4221050583
TLDR: A deep analysis of OCR errors that impact the performance of NER and NEL is provided and subsequent recommendations on the adequate documents, the OCR quality levels, and the post-OCR correction strategies required to perform reliable NERand NEL are presented.},
	keywords = {Document indexing, Information retrieval, Named entity recognition and linking, Neural networks, Optical character recognition},
	pages = {425--448},
}

@article{sporici_improving_2020,
	title = {Improving the accuracy of tesseract 4.0 {OCR} engine using convolution-based preprocessing},
	volume = {12},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2073-8994},
	url = {https://www.mdpi.com/2073-8994/12/5/715},
	doi = {10.3390/sym12050715},
	abstract = {Optical Character Recognition (OCR) is the process of identifying and converting texts rendered in images using pixels to a more computer-friendly representation. The presented work aims to prove that the accuracy of the Tesseract 4.0 OCR engine can be further enhanced by employing convolution-based preprocessing using specific kernels. As Tesseract 4.0 has proven great performance when evaluated against a favorable input, its capability of properly detecting and identifying characters in more realistic, unfriendly images is questioned. The article proposes an adaptive image preprocessing step guided by a reinforcement learning model, which attempts to minimize the edit distance between the recognized text and the ground truth. It is shown that this approach can boost the character-level accuracy of Tesseract 4.0 from 0.134 to 0.616 (+359\% relative change) and the F1 score from 0.163 to 0.729 (+347\% relative change) on a dataset that is considered challenging by its authors.},
	language = {en},
	number = {5},
	urldate = {2025-07-17},
	journal = {Symmetry},
	author = {Sporici, Dan and Cușnir, Elena and Boiangiu, Costin-Anton},
	month = may,
	year = {2020},
	note = {Publisher: MDPI AG
MAG: 3021342370
OpenAlex: W3021342370
TLDR: An adaptive image preprocessing step guided by a reinforcement learning model is proposed, which attempts to minimize the edit distance between the recognized text and the ground truth, and it is shown that this approach can boost the character-level accuracy of Tesseract 4.0.},
	pages = {17},
}

@article{lopez_bello_medical_2019,
	title = {From medical records to research papers: a literature analysis pipeline for supporting medical genomic diagnosis processes},
	volume = {15},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {2352-9148},
	shorttitle = {From medical records to research papers},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2352914819300309},
	doi = {10.1016/j.imu.2019.100181},
	abstract = {In this paper, we introduce a framework for processing genetics and genomics literature, based on ontologies and lexical resources from the biomedical domain. The main objective is to support the diagnosis process that is done by medical geneticists who extract knowledge from published works. We constructed a pipeline that gathers several genetics- and genomics-related resources and applies natural language processing techniques, which include named entity recognition and relation extraction. Working on a corpus created from PubMed abstracts, we built a knowledge database that can be used for processing medical records written in Spanish. Given a medical record from Uruguayan healthcare patients, we show how we can map it to the database and perform graph queries for relevant knowledge paths. The framework is not an end user application, but an extensible processing structure to be leveraged by external applications, enabling software developers to streamline incorporation of the extracted knowledge.},
	language = {en},
	urldate = {2025-07-17},
	journal = {Informatics in Medicine Unlocked},
	author = {López Bello, Fernando and Naya, Hugo and Raggio, Víctor and Rosá, Aiala},
	year = {2019},
	note = {Publisher: Elsevier BV
QID: Q128114010
MAG: 2937378343
OpenAlex: W2937378343
TLDR: A framework for processing genetics and genomics literature, based on ontologies and lexical resources from the biomedical domain is introduced, to support the diagnosis process that is done by medical geneticists who extract knowledge from published works.},
	keywords = {Automated pattern recognition, Controlled vocabulary, Genomics, Medical records, Natural language processing, Publications},
	pages = {11},
}

@misc{noauthor_addon_nodate,
	title = {Addon item},
	keywords = {/unread},
}

@misc{rafi_comparing_nodate,
	title = {Comparing {SVM} and naive bayes classifiers for text categorization with wikitology as knowledge enrichment},
	url = {https://www.researchgate.net/publication/221665107_Comparing_SVM_and_Naive_Bayes_classifiers_for_text_categorization_withWikitology_as_knowledge_enrichment},
	abstract = {PDF {\textbar} The activity of labeling of documents according to their content is known as text categorization. Many experiments have been carried out to... {\textbar} Find, read and cite all the research you need on ResearchGate},
	language = {en},
	urldate = {2025-07-19},
	journal = {Researchgate},
	author = {Rafi, Muhammad and Hassan, Sundus and Shaikh, M. Shahid},
	doi = {10.1109/INMIC.2011.6151495},
	keywords = {/unread},
}

@article{chiele_reconhecimento_2015,
	title = {Reconhecimento de {Entidades} {Nomeadas} para o {Português} {Usando} o {OpenNLP}},
	url = {https://repositorio.pucrs.br/dspace/handle/10923/14040},
	language = {pt\_BR},
	urldate = {2025-07-19},
	author = {Chiele, Gabriel and Fonseca, Evandro Brasil and Vanim, Aline and Vieira, Renata},
	year = {2015},
	keywords = {/unread, ⛔ No DOI found},
}

@misc{geeksforgeeks_naive_nodate,
	title = {Naive bayes vs. {SVM} for text classification},
	url = {https://www.geeksforgeeks.org/machine-learning/naive-bayes-vs-svm-for-text-classification/},
	abstract = {Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.},
	language = {en},
	urldate = {2025-07-14},
	journal = {Geeksforgeeks},
	author = {Geeksforgeeks, Geeksforgeeks},
	note = {Section: Machine Learning},
}

@misc{ritu_understanding_2025,
	title = {Understanding document classification: a step-wise breakdown},
	shorttitle = {Understanding document classification},
	url = {https://www.docsumo.com/blogs/ocr/document-classification},
	abstract = {Learn how AI-led OCR technology automates document classification via computer vision and textual recognition techniques in real-world use cases.},
	language = {en},
	urldate = {2025-07-14},
	author = {Ritu, John},
	month = jul,
	year = {2025},
}

@misc{zelic_python_2023,
	title = {Python {OCR} tutorial: tesseract, pytesseract, and {OpenCV}},
	shorttitle = {Python {OCR} tutorial},
	url = {https://nanonets.com/blog/ocr-with-tesseract/},
	abstract = {Dive deep into OCR with Tesseract, including Pytesseract integration, training with custom data, limitations, and comparisons with enterprise solutions.},
	language = {en},
	urldate = {2025-07-14},
	journal = {Nanonets Blog {\textbar} AI Document Processing \& Workflow Automation},
	author = {Zelic, Filip and Sable, Anuj},
	month = feb,
	year = {2023},
}

@misc{howlett_parsing_2022,
	title = {Parsing structured data within {PDF} documents with apache {PDFBox}},
	url = {https://robinhowlett.com/blog/2019/11/29/parsing-structured-data-complex-pdf-layouts/},
	language = {en},
	urldate = {2025-07-14},
	author = {Howlett, Robin},
	month = aug,
	year = {2022},
}

@misc{the_apache_software_foundation_apache_2025,
	title = {Apache {OpenNLP} developer documentation},
	url = {https://opennlp.apache.org/docs/1.7.2/manual/opennlp.html},
	language = {en},
	urldate = {2025-07-17},
	author = {The Apache Software Foundation, The Apache Software Foundation},
	month = jan,
	year = {2025},
	keywords = {/unread},
}

@inproceedings{smith_overview_2007,
	address = {Curitiba, Parana, Brazil},
	title = {An overview of the tesseract {OCR} engine},
	volume = {2},
	url = {https://ieeexplore.ieee.org/abstract/document/4376991},
	doi = {10.1109/ICDAR.2007.4376991},
	abstract = {The Tesseract OCR engine, as was the HP Research Prototype in the UNLV Fourth Annual Test of OCR Accuracy, is described in a comprehensive overview. Emphasis is placed on aspects that are novel or at least unusual in an OCR engine, including in particular the line finding, features/classification methods, and the adaptive classifier.},
	language = {en},
	urldate = {2025-07-17},
	booktitle = {Ninth {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR} 2007)},
	publisher = {IEEE},
	author = {Smith, R.},
	month = sep,
	year = {2007},
	note = {ISSN: 2379-2140
MAG: 2001642682
OpenAlex: W2001642682},
	keywords = {Filters, Independent component analysis, Inspection, Open source software, Optical character recognition software, Pipelines, Prototypes, Search engines, Testing, Text recognition},
	pages = {629--633},
}

@inproceedings{lima_enhancing_2025,
	title = {Enhancing text recognition in {OCR} systems through image processing with {BSRGAN}},
	copyright = {Copyright (c)},
	url = {https://sol.sbc.org.br/index.php/sbsi/article/view/34366},
	doi = {10.5753/sbsi.2025.246551},
	abstract = {Context: Image enhancement is essential for advancing Optical Character Recognition (OCR), a technology widely applied across various Information Systems (IS) to enable accurate text extraction from scanned documents, IDs, invoices, and other document types. Problem: Despite OCR’s importance, challenges such as noise, variable illumination, and low-resolution scans often compromise recognition quality, leading to distorted and inaccurate results. These issues can impact the reliability and effectiveness of IS. Solution: This study presents a methodology to improve the quality of low-resolution images by combining image filtering techniques with OpenCV, super-resolution using the BSRGAN model, and EasyOCR for character extraction. IS Theory: The research is anchored in the Information Quality theory in IS, addressing the importance of improving input data to enhance system outputs and reliability. Method: The proposed methodology consists of two main stages. First, low-resolution images are processed using the BSRGAN super-resolution model, which enhances image quality for improved OCR performance. Then, the enhanced images are processed by an OCR system to extract and convert characters into text. Validation was conducted on three datasets: Brazilian Identity Document (BID), IIIT 5K-Word, and SVHN, simulating real-world application conditions. Summary of Results: The results demonstrate the proposed methodology’s effectiveness in enhancing OCR accuracy, significantly reducing error rates in various contexts. Contributions and Impact on IS: This work contributes to the IS field by providing a solution that enhances OCR input quality, benefiting academia through advanced image processing research and the industry by enabling more reliable text recognition in practical applications.},
	language = {en},
	urldate = {2025-07-19},
	booktitle = {Simpósio {Brasileiro} {De} {Sistemas} {De} {Informação} ({SBSI})},
	publisher = {SBC},
	author = {Lima, Fernando Baptistella de and Silva, Eraylson Galdino da},
	month = may,
	year = {2025},
	note = {ISSN: 0000-0000
TLDR: This study presents a methodology to improve the quality of low-resolution images by combining image filtering techniques with OpenCV, super-resolution using the BSRGAN model, and EasyOCR for character extraction, demonstrating the proposed methodology’s effectiveness in enhancing OCR accuracy.},
	keywords = {/unread, Image Processing, OCR},
	pages = {497--505},
}

@misc{udesc_atividades_nodate,
	title = {Atividades complementares - {UDESC} {CEAD}},
	url = {https://www.udesc.br/cead/seg/ac},
	language = {pt\_BR},
	urldate = {2025-07-14},
	journal = {Centro de Educação a Distância},
	author = {UDESC, UDESC},
}

@misc{openkm_repositorio_2025,
	title = {Repositório github - openkm/document-management-system},
	copyright = {GPL-2.0},
	url = {https://github.com/openkm/document-management-system},
	abstract = {OpenKM is a Open Source Document Management System},
	urldate = {2025-07-14},
	publisher = {Open Source Document Management System (DMS) {\textbar} OpenKM},
	author = {openkm, openkm},
	month = jul,
	year = {2025},
	note = {original-date: 2016-07-11T10:25:35Z},
	keywords = {content, dms, document, gwt, java, knowledge, management, open, record, workflow},
}

@misc{dhruv_repositorio_2025,
	title = {Repositório github - darkn3to/pdfocr - a spring boot app},
	url = {https://github.com/darkn3to/pdfocr},
	abstract = {A simple Spring Boot application to convert image-based PDFs to text-embedded PDFs.},
	urldate = {2025-07-14},
	author = {Dhruv, Dhruv},
	month = may,
	year = {2025},
	note = {original-date: 2024-11-09T07:57:55Z},
	keywords = {hocr, ocr, pdf, tesseract-ocr},
}

@misc{bayde_ribeiro_sistemas_nodate,
	title = {Sistemas e mídias digitais - atividades complementares - universidade federal do ceará},
	url = {https://smd.ufc.br/pt/alunos/atividades-complementares/},
	abstract = {Site oficial dos cursos Sistemas e Mídias Digitais (Integral-Diurno e Noturno) da Universidade Federal do Ceará},
	language = {en},
	urldate = {2025-07-14},
	journal = {Sistemas E Mídias Digitais},
	author = {Bayde Ribeiro, Levi and Franklin Bonates, Mara},
}

@misc{zhang_document_2024,
	title = {Document parsing unveiled: techniques, challenges, and prospects for structured information extraction},
	shorttitle = {Document parsing unveiled},
	url = {http://arxiv.org/abs/2410.21169},
	doi = {10.48550/arXiv.2410.21169},
	abstract = {Document parsing is essential for converting unstructured and semi-structured documents-such as contracts, academic papers, and invoices-into structured, machine-readable data. Document parsing extract reliable structured data from unstructured inputs, providing huge convenience for numerous applications. Especially with recent achievements in Large Language Models, document parsing plays an indispensable role in both knowledge base construction and training data generation. This survey presents a comprehensive review of the current state of document parsing, covering key methodologies, from modular pipeline systems to end-to-end models driven by large vision-language models. Core components such as layout detection, content extraction (including text, tables, and mathematical expressions), and multi-modal data integration are examined in detail. Additionally, this paper discusses the challenges faced by modular document parsing systems and vision-language models in handling complex layouts, integrating multiple modules, and recognizing high-density text. It emphasizes the importance of developing larger and more diverse datasets and outlines future research directions.},
	language = {en},
	urldate = {2025-07-14},
	publisher = {arXiv},
	author = {Zhang, Qintong and Huang, Victor Shea-Jay and Wang, Bin and Zhang, Junyuan and Wang, Zhengren and Liang, Hao and Wang, Shawn and Lin, Matthieu and He, Conghui and Zhang, Wentao},
	month = oct,
	year = {2024},
	note = {arXiv:2410.21169 [cs]
version: 2
OpenAlex: W4404354175},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Multimedia},
}

@misc{hu_lora_2021,
	title = {{LoRA}: low-rank adaptation of large language models},
	shorttitle = {{LoRA}},
	url = {http://arxiv.org/abs/2106.09685},
	doi = {10.48550/arXiv.2106.09685},
	abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
	language = {en},
	urldate = {2025-07-14},
	publisher = {arXiv},
	author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	month = oct,
	year = {2021},
	note = {arXiv:2106.09685 [cs]
MAG: 3168867926
OpenAlex: W3168867926},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{sculley_hidden_2015,
	title = {Hidden technical debt in machine learning systems},
	volume = {28},
	url = {https://papers.nips.cc/paper_files/paper/2015/hash/86df7dcfd896fcaf2674f757a2463eba-Abstract.html},
	abstract = {Machine learning offers a fantastically powerful toolkit for building useful complexprediction systems quickly. This paper argues it is dangerous to think ofthese quick wins as coming for free. Using the software engineering frameworkof technical debt, we find it is common to incur massive ongoing maintenancecosts in real-world ML systems. We explore several ML-specific risk factors toaccount for in system design. These include boundary erosion, entanglement,hidden feedback loops, undeclared consumers, data dependencies, configurationissues, changes in the external world, and a variety of system-level anti-patterns.},
	language = {en},
	urldate = {2025-07-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Sculley, D. and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael and Crespo, Jean-François and Dennison, Dan},
	year = {2015},
	note = {MAG: 2189162242
OpenAlex: W2189162242},
	keywords = {⛔ No DOI found},
}

@misc{ma_comprehensive_2025,
	title = {A comprehensive survey on vector database: storage and retrieval technique, challenge},
	shorttitle = {A comprehensive survey on vector database},
	url = {http://arxiv.org/abs/2310.11703},
	doi = {10.48550/arXiv.2310.11703},
	abstract = {Vector databases (VDBs) have emerged to manage high-dimensional data that exceed the capabilities of traditional database management systems, and are now tightly integrated with large language models as well as widely applied in modern artificial intelligence systems. Although relatively few studies describe existing or introduce new vector database architectures, the core technologies underlying VDBs, such as approximate nearest neighbor search, have been extensively studied and are well documented in the literature. In this work, we present a comprehensive review of the relevant algorithms to provide a general understanding of this booming research area. Specifically, we first provide a review of storage and retrieval techniques in VDBs, with detailed design principles and technological evolution. Then, we conduct an in-depth comparison of several advanced VDB solutions with their strengths, limitations, and typical application scenarios. Finally, we also outline emerging opportunities for coupling VDBs with large language models, including open research problems and trends, such as novel indexing strategies. This survey aims to serve as a practical resource, enabling readers to quickly gain an overall understanding of the current knowledge landscape in this rapidly developing area.},
	language = {en},
	urldate = {2025-07-14},
	publisher = {arXiv},
	author = {Ma, Le and Zhang, Ran and Han, Yikun and Yu, Shirui and Wang, Zaitian and Ning, Zhiyuan and Zhang, Jinghan and Xu, Ping and Li, Pengjiang and Ju, Wei and Chen, Chong and Wang, Dongjie and Liu, Kunpeng and Wang, Pengyang and Wang, Pengfei and Fu, Yanjie and Liu, Chunjiang and Zhou, Yuanchun and Lu, Chang-Tien},
	month = jun,
	year = {2025},
	note = {arXiv:2310.11703 [cs]
OpenAlex: W4387800467
TLDR: This work presents a comprehensive review of the relevant algorithms in VDBs, with detailed design principles and technological evolution, and conducts an in-depth comparison of several advanced VDB solutions with their strengths, limitations, and typical application scenarios.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Databases},
}

@inproceedings{silva_etiquetagem_2023,
	address = {Brasil},
	title = {Etiquetagem morfossintática multigênero para o português do brasil segundo o modelo "universal dependencies"},
	copyright = {Copyright (c)},
	url = {https://sol.sbc.org.br/index.php/stil/article/view/25438},
	doi = {10.5753/stil.2023.233848},
	abstract = {Part of speech tagging is a process that seeks to identify the grammatical classes of words and symbols (tokens) in a sentence. For Brazilian Portuguese, there is a variety of approaches using corpora of the journalistic genre with different tagsets. In this paper, we present results better than the current state of the art, investigating tagging methods and evaluating their ability to perform multi-genre analysis in corpora of journalistic, academic and user-generated content genres. To do so, we use the Universal Dependencies model. Finally, we present a qualitative assessment of the systematic tagging errors made in the process.},
	language = {en},
	urldate = {2025-07-17},
	booktitle = {Simpósio {Brasileiro} {De} {Tecnologia} {Da} {Informação} {E} {Da} {Linguagem} {Humana} ({STIL})},
	publisher = {SBC},
	author = {Silva, Emanuel Huber and Pardo, Thiago Alexandre Salgueiro and Roman, Norton Trevisan},
	month = sep,
	year = {2023},
	note = {ISSN: 0000-0000
OpenAlex: W4386915891},
	pages = {63--73},
}

@inproceedings{vargas_socrates_2021,
	address = {Brasil},
	title = {{sOCRates} - a post-{OCR} text correction method},
	copyright = {Copyright (c)},
	url = {https://sol.sbc.org.br/index.php/sbbd/article/view/17866},
	doi = {10.5753/sbbd.2021.17866},
	abstract = {A significant portion of the textual information of interest to an organization is stored in PDF files that should be converted into plain text before their contents can be processed by an information retrieval or text mining system. When the PDF documents consist of scanned documents, optical character recognition (OCR) is typically used to extract the textual contents. OCR errors can have a negative impact on the quality of information retrieval systems since the terms in the query will not match incorrectly extracted terms in the documents. This work introduces sOCRates, a post-OCR text correction method that relies on contextual word embeddings and on a classifier that uses format, semantic, and syntactic features. Our experimental evaluation on a test collection in Portuguese showed that sOCRates can accurately correct errors and improve retrieval results.},
	language = {en},
	urldate = {2025-07-17},
	booktitle = {Simpósio {Brasileiro} {De} {Banco} {De} {Dados} ({SBBD})},
	publisher = {SBC},
	author = {Vargas, Danny Suarez and Oliveira, Lucas Lima de and Moreira, Viviane P. and Bazzo, Guilherme Torresan and Lorentz, Gustavo Acauan},
	month = oct,
	year = {2021},
	note = {ISSN: 2763-8979
OpenAlex: W4200413077
TLDR: This work introduces sOCRates, a post-OCR text correction method that relies on contextual word embeddings and on a classifier that uses format, semantic, and syntactic features to accurately correct errors and improve retrieval results.},
	pages = {61--72},
}

@phdthesis{leme_classificacao_2021,
	type = {dissertação},
	title = {Classificação automática de documentos de características econômicas para defesa jurídica},
	copyright = {publico},
	url = {https://www.teses.usp.br/teses/disponiveis/45/45134/tde-05082021-152340/},
	abstract = {Law is one of the areas benefited by the advance of Artificial Intelligence, through the automatization of relevant tasks such as outcome prediction, due dilligence, document review and intellectual property analysis. The Administrative Council of Economic Defense (CADE), an entity under the Ministry of Justice of Federal Government of Brazil, has the objective of ensure the free market competition on brazilian national territory. One of its attributions is given by the evaluation and approving, or disapproving, of merger cases, that must be submited for approval, by the group of envolved economic agents, when the operation meets specific requiriments. One of the first tasks in this process is given by the classification of the legal procedural rite the proccess must folow, it could be summary or ordinary, according with its complexity. The automatization of this task can result in less bureaucracy, due the shorter time of evaluation of the entire proccess. This research aims to evaluate machine learning techniques, as well as deep learning techniques, which have shown relevant improvements in several natural language processing tasks, to build automatic classification models to predict the most appropriated legal proccess rite a merger case must follow. We split the problem in two big challenges: (i) word and document embeddings and (iii) supervised learning of the appropriated legal proccess rite.},
	language = {en},
	urldate = {2025-07-17},
	school = {Universidade de São Paulo},
	author = {Leme, Bruno},
	month = may,
	year = {2021},
	doi = {10.11606/D.45.2021.tde-05082021-152340},
	keywords = {/unread},
}

@misc{kiss_brno_2019,
	title = {Brno mobile {OCR} dataset},
	url = {http://arxiv.org/abs/1907.01307},
	doi = {10.48550/arXiv.1907.01307},
	abstract = {We introduce the Brno Mobile OCR Dataset (B-MOD) for document Optical Character Recognition from low-quality images captured by handheld mobile devices. While OCR of high-quality scanned documents is a mature field where many commercial tools are available, and large datasets of text in the wild exist, no existing datasets can be used to develop and test document OCR methods robust to non-uniform lighting, image blur, strong noise, built-in denoising, sharpening, compression and other artifacts present in many photographs from mobile devices. This dataset contains 2 113 unique pages from random scientific papers, which were photographed by multiple people using 23 different mobile devices. The resulting 19 728 photographs of various visual quality are accompanied by precise positions and text annotations of 500k text lines. We further provide an evaluation methodology, including an evaluation server and a testset with non-public annotations. We provide a state-of-the-art text recognition baseline build on convolutional and recurrent neural networks trained with Connectionist Temporal Classification loss. This baseline achieves 2 \%, 22 \% and 73 \% word error rates on easy, medium and hard parts of the dataset, respectively, confirming that the dataset is challenging. The presented dataset will enable future development and evaluation of document analysis for low-quality images. It is primarily intended for line-level text recognition, and can be further used for line localization, layout analysis, image restoration and text binarization.},
	language = {en},
	urldate = {2025-07-17},
	publisher = {arXiv},
	author = {Kišš, Martin and Hradiš, Michal and Kodym, Oldřich},
	month = jul,
	year = {2019},
	note = {arXiv:1907.01307 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
}

@inproceedings{silva_segmentacao_2024,
	address = {Brasil},
	title = {Segmentação textual baseada em tópicos em português utilizando {BERTimbau}},
	copyright = {Copyright (c)},
	url = {https://sol.sbc.org.br/index.php/stil/article/view/31113},
	doi = {10.5753/stil.2024.245080},
	abstract = {Neste trabalho, exploramos a segmentação textual para o português utilizando o modelo BERTimbau, com bases de dados construídas usando tradução automática e a partir de notícias online. Obtivemos Pk = 6,89 para uma avaliação dentro do domínio, mas resultados piores em avaliações fora do domínio, destacando a importância de uma base de treinamento diversificada para melhorar a generalização em múltiplos domínios.},
	language = {en},
	urldate = {2025-07-17},
	booktitle = {Simpósio {Brasileiro} {De} {Tecnologia} {Da} {Informação} {E} {Da} {Linguagem} {Humana} ({STIL})},
	publisher = {SBC},
	author = {Silva, Luciano A. C. da and Rodrigues, Maiara S. F. and Archanjo, Adriana P. and Pessoa, Luis and Silva, Miguel L. and Almeida, Thiago F. de and Silveira, Leonardo},
	month = nov,
	year = {2024},
	note = {ISSN: 0000-0000
OpenAlex: W4405194276},
	pages = {5},
}

@inproceedings{lopes_syntactic_2024,
	title = {Syntactic parsing: where are we going?},
	copyright = {Copyright (c)},
	shorttitle = {Syntactic parsing},
	url = {https://sol.sbc.org.br/index.php/stil/article/view/31117},
	doi = {10.5753/stil.2024.245043},
	abstract = {In this review \&amp; opinion paper, we discuss the options and challenges for syntactic parsing. Despite significant advances in recent years, driven primarily by neural network architectures, parsing accuracy appears to be approaching a plateau. This paper proposes a reflection on the factors that may possibly be influencing such results and suggests some future paths.},
	language = {en},
	urldate = {2025-07-17},
	booktitle = {Simpósio {Brasileiro} {De} {Tecnologia} {Da} {Informação} {E} {Da} {Linguagem} {Humana} ({STIL})},
	publisher = {SBC},
	author = {Lopes, Lucelene and Pardo, Thiago Alexandre Salgueiro and Duran, Magali S.},
	month = nov,
	year = {2024},
	note = {ISSN: 0000-0000
TLDR: A reflection on the factors that may possibly be influencing parsing accuracy results and some future paths are suggested.},
	pages = {67--74},
}

@misc{setic-ufsc_sistemas_nodate,
	title = {Sistemas de informação},
	url = {https://sin.ufsc.br/procedimentos-2/atividades-complementares/},
	abstract = {Curso de Graduação em Sistemas de Informação},
	language = {en},
	urldate = {2025-07-17},
	author = {SeTIC-UFSC},
}

@misc{occhipinti_pipeline_2022,
	title = {A pipeline and comparative study of 12 machine learning models for text classification},
	url = {http://arxiv.org/abs/2204.06518},
	doi = {10.48550/arXiv.2204.06518},
	abstract = {Text-based communication is highly favoured as a communication method, especially in business environments. As a result, it is often abused by sending malicious messages, e.g., spam emails, to deceive users into relaying personal information, including online accounts credentials or banking details. For this reason, many machine learning methods for text classification have been proposed and incorporated into the services of most email providers. However, optimising text classification algorithms and finding the right tradeoff on their aggressiveness is still a major research problem. We present an updated survey of 12 machine learning text classifiers applied to a public spam corpus. A new pipeline is proposed to optimise hyperparameter selection and improve the models' performance by applying specific methods (based on natural language processing) in the preprocessing stage. Our study aims to provide a new methodology to investigate and optimise the effect of different feature sizes and hyperparameters in machine learning classifiers that are widely used in text classification problems. The classifiers are tested and evaluated on different metrics including F-score (accuracy), precision, recall, and run time. By analysing all these aspects, we show how the proposed pipeline can be used to achieve a good accuracy towards spam filtering on the Enron dataset, a widely used public email corpus. Statistical tests and explainability techniques are applied to provide a robust analysis of the proposed pipeline and interpret the classification outcomes of the 12 machine learning models, also identifying words that drive the classification results. Our analysis shows that it is possible to identify an effective machine learning model to classify the Enron dataset with an F-score of 94\%.},
	language = {en},
	urldate = {2025-07-17},
	publisher = {arXiv},
	author = {Occhipinti, Annalisa and Rogers, Louis and Angione, Claudio},
	month = apr,
	year = {2022},
	note = {arXiv:2204.06518 [cs]
OpenAlex: W4302773768
CorpusID: 248020006},
	keywords = {/unread, Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Machine Learning},
}

@inproceedings{huang_icdar2019_2019,
	title = {{ICDAR2019} competition on scanned receipt {OCR} and information extraction},
	url = {http://arxiv.org/abs/2103.10213},
	doi = {10.1109/ICDAR.2019.00244},
	abstract = {Scanned receipts OCR and key information extraction (SROIE) represent the processeses of recognizing text from scanned receipts and extracting key texts from them and save the extracted tests to structured documents. SROIE plays critical roles for many document analysis applications and holds great commercial potentials, but very little research works and advances have been published in this area. In recognition of the technical challenges, importance and huge commercial potentials of SROIE, we organized the ICDAR 2019 competition on SROIE. In this competition, we set up three tasks, namely, Scanned Receipt Text Localisation (Task 1), Scanned Receipt OCR (Task 2) and Key Information Extraction from Scanned Receipts (Task 3). A new dataset with 1000 whole scanned receipt images and annotations is created for the competition. In this report we will presents the motivation, competition datasets, task definition, evaluation protocol, submission statistics, performance of submitted methods and results analysis.},
	language = {en},
	urldate = {2025-07-18},
	author = {Huang, Zheng and Chen, Kai and He, Jianhua and Bai, Xiang and Karatzas, Dimosthenis and Lu, Shjian and Jawahar, C. V.},
	month = sep,
	year = {2019},
	note = {arXiv:2103.10213 [cs]
TLDR: The ICDAR 2019 Challenge on "Scanned receipts OCR and key information extraction" (SROIE) covers important aspects related to the automated analysis of scanned receipts, and is considered to evolve into a useful resource for the community, drawing further attention and promoting research and development efforts in this field.},
	keywords = {/unread, Computer Science - Artificial Intelligence},
}

@article{fernandez-blanco_design_2025,
	title = {Design, implementation and practical energy-efficiency evaluation of a blockchain based academic credential verification system for low-power nodes},
	volume = {15},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/15/12/6596},
	doi = {10.3390/app15126596},
	abstract = {The educational system manages extensive documentation and paperwork, which can lead to human errors and sometimes abuse or fraud, such as the falsification of diplomas, certificates or other credentials. In fact, in recent years, multiple cases of fraud have been detected, representing a significant cost to society, since fraud harms the trustworthiness of certificates and academic institutions. To tackle such an issue, this article proposes a solution aimed at recording and verifying academic records through a decentralized application that is supported by a smart contract deployed in the Ethereum blockchain and by a decentralized storage system based on Inter-Planetary File System (IPFS). The proposed solution is evaluated in terms of performance and energy efficiency, comparing the results obtained with a traditional Proof-of-Work (PoW) consensus protocol and the new Proof-of-Authority (PoA) protocol. The results shown in this paper indicate that the latter is clearly greener and demands less CPU load. Moreover, this article compares the performance of a traditional computer and two Single-Board Computers (SBCs) (a Raspberry Pi 4 and an Orange Pi One), showing that is possible to make use of the latter low-power devices to implement blockchain nodes but at the cost of higher response latency. Furthermore, the impact of Ethereum gas limit is evaluated, demonstrating its significant influence on the blockchain network performance. Thus, this article provides guidelines, useful practical evaluations and key findings that will help the next generation of green blockchain developers and researchers.},
	language = {en},
	number = {12},
	urldate = {2025-07-17},
	journal = {Applied Sciences},
	author = {Fernández-Blanco, Gabriel and Froiz-Míguez, Iván and Fraga-Lamas, Paula and Fernández-Caramés, Tiago M.},
	month = jun,
	year = {2025},
	note = {Publisher: MDPI AG
OpenAlex: W4411249039
TLDR: This article proposes a solution aimed at recording and verifying academic records through a decentralized application supported by a smart contract deployed in the Ethereum blockchain and by a decentralized storage system based on Inter-Planetary File System (IPFS).},
	pages = {42},
}

@article{uddin_comparing_2019,
	title = {Comparing different supervised machine learning algorithms for disease prediction},
	volume = {19},
	copyright = {http://creativecommons.org/licenses/by/4.0/},
	issn = {1472-6947},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6925840/},
	doi = {10.1186/s12911-019-1004-8},
	abstract = {Background
Supervised machine learning algorithms have been a dominant method in the data mining field. Disease prediction using health data has recently shown a potential application area for these methods. This study ai7ms to identify the key trends among different types of supervised machine learning algorithms, and their performance and usage for disease risk prediction.

Methods
In this study, extensive research efforts were made to identify those studies that applied more than one supervised machine learning algorithm on single disease prediction. Two databases (i.e., Scopus and PubMed) were searched for different types of search items. Thus, we selected 48 articles in total for the comparison among variants supervised machine learning algorithms for disease prediction.

Results
We found that the Support Vector Machine (SVM) algorithm is applied most frequently (in 29 studies) followed by the Naïve Bayes algorithm (in 23 studies). However, the Random Forest (RF) algorithm showed superior accuracy comparatively. Of the 17 studies where it was applied, RF showed the highest accuracy in 9 of them, i.e., 53\%. This was followed by SVM which topped in 41\% of the studies it was considered.

Conclusion
This study provides a wide overview of the relative performance of different variants of supervised machine learning algorithms for disease prediction. This important information of relative performance can be used to aid researchers in the selection of an appropriate supervised machine learning algorithm for their studies.},
	language = {en},
	number = {1},
	urldate = {2025-07-17},
	journal = {BMC Medical Informatics and Decision Making},
	author = {Uddin, Shahadat and Khan, Arif and Hossain, Md Ekramul and Moni, Mohammad Ali},
	month = dec,
	year = {2019},
	pmid = {31864346},
	pmcid = {PMC6925840},
	note = {MAG: 2995098893
OpenAlex: W2995098893
QID: Q92172784
TLDR: It is found that the Support Vector Machine (SVM) algorithm is applied most frequently (in 29 studies) followed by the Naïve Bayes algorithm (in 23 studies), however, the Random Forest algorithm showed superior accuracy comparatively.},
	pages = {26},
}

@article{taha_comprehensive_2024,
	title = {A comprehensive survey of text classification techniques and their research applications: observational and experimental insights},
	volume = {54},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {1574-0137},
	shorttitle = {A comprehensive survey of text classification techniques and their research applications},
	url = {http://arxiv.org/abs/2401.12982},
	doi = {10.1016/j.cosrev.2024.100664},
	abstract = {The exponential growth of textual data presents substantial challenges in management and analysis, notably due to high storage and processing costs. Text classification, a vital aspect of text mining, provides robust solutions by enabling efficient categorization and organization of text data. These techniques allow individuals, researchers, and businesses to derive meaningful patterns and insights from large volumes of text. This survey paper introduces a comprehensive taxonomy specifically designed for text classification based on research fields. The taxonomy is structured into hierarchical levels: research field-based category, research field-based sub-category, methodology-based technique, methodology sub-technique, and research field applications. We employ a dual evaluation approach: empirical and experimental. Empirically, we assess text classification techniques across four critical criteria. Experimentally, we compare and rank the methodology sub-techniques within the same methodology technique and within the same overall research field sub-category. This structured taxonomy, coupled with thorough evaluations, provides a detailed and nuanced understanding of text classification algorithms and their applications, empowering researchers to make informed decisions based on precise, field-specific insights.},
	language = {en},
	urldate = {2025-07-17},
	journal = {Computer Science Review},
	author = {Taha, Kamal and Yoo, Paul D. and Yeun, Chan and Taha, Aya},
	month = nov,
	year = {2024},
	note = {arXiv:2401.12982 [cs]
OpenAlex: W4401836561
TLDR: A comprehensive taxonomy specifically designed for text classification based on research fields, coupled with thorough evaluations, provides a detailed and nuanced understanding of text classification algorithms and their applications, empowering researchers to make informed decisions based on precise, field-specific insights.},
	keywords = {Computer Science - Computation and Language},
	pages = {23},
}

@article{kouamou_structural_2017,
	title = {A structural and generative approach to multilayered software architectures},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/4.0/},
	issn = {1945-3116, 1945-3124},
	url = {https://www.scirp.org/journal/paperinformation?paperid=77545},
	doi = {10.4236/jsea.2017.108037},
	abstract = {The layered software architecture is the model commonly adopted for the development of information systems since it favors the modularity and the scalability of the systems. On the other hand, the emergence of model engineering aims to raise the level of abstraction to allow developers to reason on models, and less in code. The research question is to combine the two approaches to facilitate the work of developers. The proposal resulting from this study is based on a set of concepts defined using the UML profiles. These concepts include services, business components, and data persistence. Then the Kruchten model is adopted to represent the development cycle according to several views, each view being represented by UML diagrams derived from the previously defined profiles. Finally, rules are available for checking inter-view consistency, from refinement to code generation. The result is a step towards the definition of a domain specific ADL and a development process as much as it includes the expected characteristics of such a language, namely: the fundamental concepts, the support tools and the multiview development.},
	language = {en},
	number = {8},
	urldate = {2025-07-17},
	journal = {Journal of Software Engineering and Applications},
	author = {Kouamou, Georges Edouard and Kungne, Willy Kengne},
	month = jul,
	year = {2017},
	note = {Number: 8
Publisher: Scientific Research Publishing
MAG: 2735274012
OpenAlex: W2735274012
TLDR: The proposal resulting from this study is a step towards the definition of a domain specific ADL and a development process as much as it includes the expected characteristics of such a language, namely: the fundamental concepts, the support tools and the multiview development.},
	pages = {677--692},
}

@article{devlin_bert_2019,
	title = {{BERT}: pre-training of deep bidirectional transformers for language understanding},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Bert},
	url = {http://arxiv.org/abs/1810.04805},
	doi = {10.48550/arXiv.1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	language = {en},
	urldate = {2025-07-17},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {176 citations (INSPIRE 2025/7/14)
176 citations w/o self (INSPIRE 2025/7/14)
arXiv:1810.04805 [cs.CL]
MAG: 2896457183
OpenAlex: W2896457183
QID: Q57267388},
	keywords = {Computer Science - Computation and Language},
}

@article{ferreira_processamento_2019,
	title = {Processamento de linguagem natural e classificação de textos em sistemas modulares},
	url = {https://bdm.unb.br/handle/10483/25114},
	abstract = {Sistemas modulares são implementados de forma que cada componente possa, individualmente, alcançar seus objetivos e contribuir para o correto funcionamento do sistema. A extração de dados textuais de fontes online pode ter alterações e depende de fornecer meios simples de modificar apenas módulos individuais responsáveis por estas tarefas, adequando-se às atualizações das fontes de dados sem afetar o resto do sistema. Os processos devem estar adequados à dinâmica do ambiente que estão disponíveis, visando escalabilidade e processamento de forma eficiente. O ambiente é estocástico e força os diferentes módulos a serem o mais completos e generalistas possível e seus componentes facilmente manuteníveis. A língua portuguesa também é um grande desafio, devido sua heterogeneidade, a diversidade de fontes e de modos de escrita, este trabalho busca encontrar padrões e metodologias de normalização e limpeza de dados que sirvam também a outros contextos e a outras línguas. Esse trabalho propõe uma arquitetura com diversos módulos que realizem tarefas de captura de dados textuais, fluxos de pré-processamento de dados, extração de entidades de textos em linguagem natural, estruturação e formatação dos dados, armazenamento destes dados de forma eficiente e resiliente, processamento de linguagem natural, classificação de textos em na língua portuguesa. Esta arquitetura se baseia em um fluxo completo que contempla a obtenção, processamento e análise dos dados. Este trabalho também visa aplicar suas metodologias sobre dados governamentais, buscando gerar insumos para a identificação de comportamento de cartéis de empresas em obras públicas por meio de técnicas de aprendizado de máquina e inteligência artificial. Experimentos indicam resultados positivos para a estruturação de uma arquitetura que possa extrair os dados e processá-los corretamente, trazendo os indícios necessários para uma análise de dados e oferecendo informações para aprofundar no campo de conhecimento de aplicação do processamento de linguagem natural em sistemas modulares e inteligentes.},
	language = {en},
	urldate = {2025-07-14},
	author = {Ferreira, Hugo Honda},
	month = mar,
	year = {2019},
	note = {Accepted: 2020-07-29T19:15:24Z
MAG: 3103449308
OpenAlex: W3103449308},
	keywords = {⛔ No DOI found},
}
